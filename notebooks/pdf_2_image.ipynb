{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation of Image dataset from PDF files\n",
    "\n",
    "This notebook loads the pdf files contained in the googledrive folders `newspaper` and `OCR_Dataset`, the files are split page by page to create image files which are then uploaded to huggingface to make the dataset readily available.\n",
    "\n",
    "Being aware that the data contained in `newspaper` will require segementation and those in `OCR_Dataset` will not, a suitable metadata to label the dataset can easily be generated as part of the image conversion process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from newspaper_parser.utils import split_pdf2image_and_add_to_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame(columns=[\"source\", \"name\", \"for_segmentation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_dir = \"data/newspaper\"\n",
    "other_dir = \"data/OCR_Dataset\"\n",
    "\n",
    "segementation_dataset_list = []\n",
    "other_dataset_list = []\n",
    "\n",
    "# Process segmentation dataset files\n",
    "for item in os.listdir(segmentation_dir):\n",
    "    full_path = os.path.join(segmentation_dir, item)\n",
    "\n",
    "    if os.path.isfile(full_path):\n",
    "        segementation_dataset_list.append(full_path)\n",
    "\n",
    "for item in os.listdir(other_dir):\n",
    "    full_path = os.path.join(other_dir, item)\n",
    "\n",
    "    if os.path.isfile(full_path):\n",
    "        other_dataset_list.append(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_list = [\n",
    "    \"data/newspaper/newspaper_24.pdf\",\n",
    "    \"data/newspaper/newspaper_33.pdf\",\n",
    "    \"data/newspaper/newspaper_33.pdf\",\n",
    "    \"data/OCR_Dataset/Alo_4.pdf\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in segementation_dataset_list:\n",
    "    if item not in test_dataset_list:\n",
    "        split_pdf2image_and_add_to_dataframe(\n",
    "            data_path=item, df=dataset, for_segmentation=True\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        split_pdf2image_and_add_to_dataframe(\n",
    "            data_path=item, df=dataset, for_segmentation=True, train_or_test=\"test\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in other_dataset_list:\n",
    "    if item not in test_dataset_list:\n",
    "        split_pdf2image_and_add_to_dataframe(\n",
    "            data_path=item, df=dataset, for_segmentation=False\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        split_pdf2image_and_add_to_dataframe(\n",
    "            data_path=item, df=dataset, for_segmentation=False, train_or_test=\"test\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1132 entries, 0 to 1131\n",
      "Data columns (total 3 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   source            1132 non-null   object\n",
      " 1   name              1132 non-null   object\n",
      " 2   for_segmentation  1132 non-null   int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 35.4+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "for_segmentation\n",
       "0    599\n",
       "1    533\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.for_segmentation.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. FineTune a ViT Model for Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dugerij/Library/Caches/pypoetry/virtualenvs/newspaper-parser-AcAJTpOj-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 935/935 [00:00<00:00, 61529.97files/s]\n",
      "Downloading data: 100%|██████████| 197/197 [00:00<00:00, 635598.38files/s]\n",
      "Generating train split: 935 examples [00:00, 15283.22 examples/s]\n",
      "Generating test split: 197 examples [00:00, 23097.16 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample from biglam/europeana_newspapers Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"biglam/europeana_newspapers\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newspaper-parser-AcAJTpOj-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
